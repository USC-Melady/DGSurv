{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def result_file_name(args):\n",
    "    file_name = f\"results_{args.fold_range}_{args.seed}\"\n",
    "    if len(args.modalities) > 0:\n",
    "        file_name += \"_\"\n",
    "        file_name += \"_\".join(args.modalities)\n",
    "    return file_name\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, modalities=[\"clinical\", \"miRNA\", \"mRNA\", \"WSI\"]):\n",
    "        self.dataset = 'kidney'\n",
    "        self.modality_data_path = {'clinical': '../preprocess/preprocessed_data/clinical_kidney.csv',\n",
    "                                    'mRNA': '../preprocess/preprocessed_data/mrna_kidney.csv',\n",
    "                                    'miRNA': '../preprocess/preprocessed_data/mirna_kidney.csv',\n",
    "                                    'WSI': '../preprocess/preprocessed_data/UNI2_features/TCGA-Kidney.pt'}\n",
    "        self.device = \"cuda\"\n",
    "        self.modalities = modalities\n",
    "        self.input_modality_dim = {'clinical':4, 'mRNA':2746, 'miRNA':743 , 'WSI': 1536}\n",
    "        self.fold_range = 5\n",
    "        self.fold = 1\n",
    "        self.modality_fv_len = 128\n",
    "        self.batch_size = 128\n",
    "        self.seed = 24\n",
    "        self.model_path = f\"../logs/dgsurv/{result_file_name(self)}\"\n",
    "        self.num_workers = 4\n",
    "        self.num_modalities = len(self.modalities)\n",
    "        self.split_path = \"../splits/kidney_splits\"\n",
    "        self.remove_missing = True\n",
    "\n",
    "\n",
    "args = Args()\n",
    "sys.path.append(os.path.abspath(\"..\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from method.GraphLearner import GraphLearner\n",
    "from method.GraphPooling import GraphPooling\n",
    "from method.GraphDataUtils import GraphInputProcessor\n",
    "\n",
    "\n",
    "class GraphModel(nn.Module):\n",
    "    def __init__(self, n_views, n_in_feats, encoder_model,\n",
    "                 gnn_arch='gcn',\n",
    "                 pool_ratio=.1, sparse_threshold=0.1,\n",
    "                 device='cuda'):\n",
    "        super(GraphModel, self).__init__()\n",
    "        self.encoder_model = encoder_model\n",
    "\n",
    "        n_edge_types = int(n_views**2)\n",
    "        self.graph_input_processor = GraphInputProcessor(\n",
    "            n_edge_types=n_edge_types, device=device)\n",
    "\n",
    "        self.graph_learner = GraphLearner(\n",
    "            n_in_feats=n_in_feats+n_views,\n",
    "            n_out_feats=100,\n",
    "            threshold=sparse_threshold,\n",
    "            n_heads=1,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        n_nodes = max(math.ceil(n_views*pool_ratio), 1)\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        n_feats = n_in_feats\n",
    "        self.pool = GraphPooling(\n",
    "            n_in_feats, n_nodes,\n",
    "            n_feats, n_edge_types,\n",
    "            gnn_arch=gnn_arch, pool=True\n",
    "        )\n",
    "\n",
    "        self.output_dim = n_feats\n",
    "        self.hazard_layer1 = nn.Linear(self.output_dim, 1)\n",
    "\n",
    "        self.label_layer1 = nn.Linear(self.output_dim, 2)\n",
    "\t\t\t\n",
    "    def forward(self, x_modality, mask):\n",
    "        representation = self.encoder_model(x_modality)\n",
    "        missing_rep = []\n",
    "        for i, rep in enumerate(representation):\n",
    "            index = torch.ones((rep.shape[0]), dtype=int) * i \n",
    "            index = index.to(rep.device)\n",
    "            modality_mask = mask[:, i].reshape(-1, 1)\n",
    "            rep =  modality_mask * rep\n",
    "            missing_rep.append(rep)\n",
    "        \n",
    "        representation_dict = {} #used for self supervised loss\n",
    "        for i, data in enumerate(missing_rep):\n",
    "            representation_dict[i] = data\n",
    "                     \n",
    "        in_graph = self.graph_input_processor(missing_rep)\n",
    "        learned_graph, _ = self.graph_learner(in_graph)\n",
    "        latent_graph, assignment_mat = self.pool(learned_graph)\n",
    "        final_representation = latent_graph[0].squeeze(1)\n",
    "        hazard = self.hazard_layer1(final_representation)\n",
    "        score = F.log_softmax(self.label_layer1(final_representation), dim=1)\n",
    "        return {'hazard':hazard, 'score':score}, representation_dict, learned_graph[1], assignment_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "from utils.encoder import EncoderModel\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.dataset import MultiModalDataset\n",
    "\n",
    "\n",
    "class ClinicalEmbedder(nn.Module):\n",
    "\tdef __init__(self, m_length, n_continuous=1, embedding_size=[(33, 2), (2, 1), (6, 3), (145, 2)]):\n",
    "\t\tsuper(ClinicalEmbedder, self).__init__()\n",
    "  \t\t# Embedding Layer\n",
    "\t\tself.embedding_layers = nn.ModuleList([nn.Embedding(categories, size)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tfor categories, size in embedding_size])\n",
    "\t\tn_emb = sum(e.embedding_dim for e in self.embedding_layers)\n",
    "\t\tself.n_emb, self.n_continuous = n_emb, n_continuous\n",
    "\t\t# Linear Layer\n",
    "\t\tself.hidden1 = nn.Linear(self.n_emb + self.n_continuous, m_length)\n",
    "\t\t# batch normalization\n",
    "\t\tself.bn1 = nn.BatchNorm1d(self.n_continuous)\n",
    "\t\tself.emb_drop = nn.Dropout(0.4)\n",
    "    \n",
    "\tdef forward(self, x_categorical, x_continuous):\n",
    "\t\tx = [e(x_categorical[:, i]) for i, e in enumerate(self.embedding_layers)]\n",
    "\t\tx = torch.cat(x, 1)\n",
    "\t\tx = self.emb_drop(x)\n",
    "\t\tx2 = self.bn1(x_continuous)\n",
    "\t\tx = torch.cat([x, x2], 1) # Note no linear layer was used in the end\n",
    "\t\treturn x\n",
    "\n",
    "class InterpEncoderModel(EncoderModel):\n",
    "\tdef __init__(self, modalities, modality_fv_len, input_modality_dim, drop_out_p=0.5):\n",
    "\t\tsuper(InterpEncoderModel, self).__init__(modalities, modality_fv_len, input_modality_dim, drop_out_p=0.5)\n",
    "\t\t\t\n",
    "\tdef forward(self, x_modality):\n",
    "\t\t# Extract representations from different modalities\n",
    "\t\trepresentation = []\n",
    "\t\tfor modality in self.data_modalities:\n",
    "\t\t\trepresentation.append(self.modality_pipeline[modality](x_modality[modality]))\n",
    "\t\treturn representation\n",
    "\n",
    "\n",
    "def embed_clinical(x_modal, clinical_embedder, modalities):\n",
    "    # Replace the clinical data with the embedding\n",
    "    clin_embed = clinical_embedder(x_modal['clinical_categorical'], x_modal['clinical_continuous'])\n",
    "    x_modal['clinical'] = clin_embed\n",
    "    x_modal.pop('clinical_categorical', None)\n",
    "    x_modal.pop('clinical_continuous', None)\n",
    "    new_x_modal = {}\n",
    "    new_x_modal['clinical'] = clin_embed\n",
    "    for key in modalities:\n",
    "        if key != 'clinical':\n",
    "            new_x_modal[key] = x_modal[key]\n",
    "    return new_x_modal\n",
    "    \n",
    "def calculate_shap_values(args, model, clinical_embedder, device):\n",
    "    train_dataset = MultiModalDataset(args, 'train', args.modalities, args.modality_data_path)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False, num_workers=args.num_workers)\n",
    "    for x_modal, _, mask in train_dataloader:\n",
    "        for modality in x_modal:\n",
    "            x_modal[modality] = x_modal[modality].to(device)\n",
    "        mask = mask.to(device)\n",
    "        x_modal = embed_clinical(x_modal, clinical_embedder, args.modalities)\n",
    "    e = shap.DeepExplainer(model, x_modal, mask)\n",
    "\n",
    "    test_dataset = MultiModalDataset(args, 'test', args.modalities, args.modality_data_path)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "\n",
    "    all_shape_values = []\n",
    "    for x_modal, _, mask in test_dataloader:\n",
    "        for modality in x_modal:\n",
    "            x_modal[modality] = x_modal[modality].to(device)\n",
    "        mask = mask.to(device)\n",
    "        x_modal = embed_clinical(x_modal, clinical_embedder, args.modalities)\n",
    "        shap_values = e.shap_values(x_modal, mask)\n",
    "        all_shape_values.append(shap_values)\n",
    "\n",
    "    cat_shap_values = [None] * len(all_shape_values[0])\n",
    "    for shap_value_list in all_shape_values:\n",
    "        for i, shap_value in enumerate(shap_value_list):\n",
    "            if cat_shap_values[i] is None:\n",
    "                cat_shap_values[i] = shap_value\n",
    "            else:\n",
    "                cat_shap_values[i] = np.concatenate((cat_shap_values[i], shap_value), axis=0)\n",
    "    return cat_shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: SumAggregation\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: Threshold\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: CosineSimilarity\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: GraphInputProcessor\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: SumAggregation\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: Threshold\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: CosineSimilarity\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: GraphInputProcessor\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: SumAggregation\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: Threshold\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: CosineSimilarity\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: GraphInputProcessor\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: SumAggregation\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: Threshold\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: CosineSimilarity\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: GraphInputProcessor\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: SumAggregation\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: Threshold\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: CosineSimilarity\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "/home/sajjad/dgsurv_amia_revision/DGSurv/interpretability/shap/explainers/_deep/deep_pytorch.py:256: UserWarning: unrecognized nn.Module: GraphInputProcessor\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kidney\n",
      "(939, 9, 1)\n",
      "(939, 743, 1)\n",
      "(939, 2746, 1)\n",
      "(939, 1536, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "all_modalities = [\"clinical\", \"miRNA\", \"mRNA\", \"WSI\"]\n",
    "all_modalities = np.array(all_modalities)\n",
    "all_shape_values = []\n",
    "model_base_path = args.model_path\n",
    "\n",
    "for fold in range(args.fold_range):\n",
    "    args = Args(all_modalities)\n",
    "    args.fold = fold\n",
    "    device = torch.device(args.device)\n",
    "    encoder_model = InterpEncoderModel(args.modalities, args.modality_fv_len, args.input_modality_dim)\n",
    "    model = GraphModel(args.num_modalities, args.modality_fv_len, encoder_model, device=args.device)  \n",
    "         \n",
    "    model_path = os.path.join(args.model_path, f\"model_fold{args.fold}.pt\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "\n",
    "    # Change the clinical_submodel to ClinicalEmbedder and LinearLayer\n",
    "    model_dict = torch.load(model_path)\n",
    "    clinical_embedder = ClinicalEmbedder(args.modality_fv_len)\n",
    "    clinical_embedder_dict = {}\n",
    "    for key in model_dict:\n",
    "        prefix = \"encoder_model.clinical_submodel\"\n",
    "        if prefix in key:\n",
    "            clinical_embedder_dict[key[len(prefix)+1:]] = model_dict[key]\n",
    "    clinical_embedder.load_state_dict(clinical_embedder_dict)\n",
    "    clinical_embedder.to(device)\n",
    "    clinical_embedder.eval()\n",
    "    model.eval()\n",
    "    model.encoder_model.clinical_submodel = clinical_embedder.hidden1\n",
    "    model.encoder_model.modality_pipeline['clinical'] = clinical_embedder.hidden1\n",
    "\n",
    "    shap_values = calculate_shap_values(args, model, clinical_embedder, device)\n",
    "    all_shape_values.append(shap_values)\n",
    "\n",
    "cat_shap_values = [None] * len(all_shape_values[0])\n",
    "for shap_value_list in all_shape_values:\n",
    "    for i, shap_value in enumerate(shap_value_list):\n",
    "        if cat_shap_values[i] is None:\n",
    "            cat_shap_values[i] = shap_value\n",
    "        else:\n",
    "            cat_shap_values[i] = np.concatenate((cat_shap_values[i], shap_value), axis=0)\n",
    "print(args.dataset)\n",
    "for shap_value in cat_shap_values:\n",
    "    print(shap_value.shape)    \n",
    "torch.save( cat_shap_values, f\"./output/{args.dataset}_shap_values.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "int_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
